# Jigsaw Unintended Bias in Toxicity Classification

## Description
In this Kaggle competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations.

## Dependencies
- Python 3, NumPy, Pandsas, Tensorflow Keras, BERT Pretrained Model
- Kaggle environment (recommended)

## Installing and Executing Program
- Download the dataset from kaggle
- Clone the git repo
- Navigate to the project 2 directory
- Open Project 2.ipynb

## Authors
Shohrab Hossen Niaz

## License
This project is licensed under the [Apache License 2.0] License - see the LICENSE.md file for details
